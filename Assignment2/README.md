# Assignment 2

All questions/subquestions have been attempted. In case of any discrepancies please contact either of the group members.<br>
We have tried our best to adhere to all the naming conventions as described in the problem statement.

## A quick walk through the files

Following is a list of the files and possible options in them :

* **src.c(code)** <br>
	Main communication/computation script: It branches out from main() using the option and optimization argument from the command line.<br>
	There are two types of functions corresponding to each collective:
	- default: This function executes the default MPI collective.
	- optimized: This function executes the modified implemenation of the MPI collective which we propose to obtain performance improvements. The optimizations and changes made are explained [below](#Optimizations).

```sh
	mpirun -np P -f hostfile ./code D option optimized   
                #P is the number of processes, D is the data size for communication
				#option is a value{1,2,3,4} which branches for various collectives covered.
                #  optimized is a value {0,1} which selects whether to run the default collective or the optimized version 
```

* **Makefile**

```sh
	make        # Builds the executable for src.c named halo
	make clean  # Removes the executables binaries created in the make process. 
```

*	**script.py**<br>
	The helper script provided to generate the hostfile

*	**plot.py**<br>
	The helper plotting script provided: Uses `data*.txt` files to generate the required plots

*	**run.sh**<br>
	The Job script: Triggers various components of the assignment(`Makefile`, `hostfile.sh`, `src.c`, `plot.py`). <br>

```sh
	bash run.sh
```

*	**data_\*.csv**<br>
	There are four data files(\*=1,2,3,4) generated by `run.sh` as the data dump corresponding to the different collectives (MPI_Bcast, MPI_Reduce, MPI_Gather, MPI_Alltoallv).

*	**plot_\*.jpg**<br>
	There are four Category plots (\*=1(BCast),2(Reduce),3(Gather),4(AlltoAllv)) generated by `run.sh` corresponding to each datafile and with various configurations.	

## Running the code


```sh
	cd Assignment2
	bash run.sh
```

Files created in this process include `code`(executable for src.c), `hostfile`, `data_*.csv` & `plot_*.jpg` .


## Optimizations
Our optimizations work by reducing the number of inter-switch or inter-group calls made by processes. For achieving this, in the optimized code for each collective, we create two times of communicators. The **intra_group** communicator includes the processes of the same group. The **inter_group** communicator includes the first process of each group (rank 0 of each intra_group communicator). Using this we describe the collective calls made to achieve the same functionality as the default MPI collective call.

* **MPI_Bcast** <br>
MPI_Bcast is first called by the first process of each group using the inter_group communicator. This is followed by a MPI_Bcast call by all processes using the intra_group communicator.

* **MPI_Reduce** <br>
MPI_Reduce is first called by all processes using the intra_group communicator. This is followed by a MPI_Reduce call by the process of each group using the inter_group communicator.

* **MPI_Gather** <br>
MPI_Gather is first called by all processes using the intra_group communicator. This is followed by a MPI_Gather call by the process of each group using the inter_group communicator.

* **MPI_Alltoallv** <br>
In this, we create additional inter_group communicators for other ranks of each group. This means there is an inter_group containing rank 0 of each group, rank 1 of each group and so on. We accumulate the data at each rank from all other ranks using MPI_Gatherv calls. For getting the data at any rank (intra_rank corresponding to intra_group communicator = r), there is a MPI_Gatherv call using intra_comm, with the root node as r. This is followed by an MPI_Gatherv call by all processes with intra_rank r using the corresponding inter_group communicator to send the data to the desired rank.

## Observations

* **MPI_Bcast** <br>
* **MPI_Reduce** <br>
* **MPI_Gather** <br>
* **MPI_Alltoallv** <br>

### Box Plots

![MPI_BCast](plot_1.jpg)
![MPI_Reduce](plot_2.jpg)
![MPI_Gather](plot_3.jpg)
![MPI_AlltoAllv](plot_4.jpg)
